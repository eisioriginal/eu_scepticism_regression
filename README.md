# eu_scepticism_regression

To keep only the relevant information, this team opted for a dictionary approach from which they filter out document irrelevant for the European integration. They identified EU resources (list of terms) available online discussing issues related to the EU and the European integration project, and constructed a dictionary containing only uni-grams and bi-grams.

The entries of the dictionary are used as a filtering process. Each speech is regarded as one instance, which consists of multiple sentences. Our filtering works at the sentence level. They used the dictionary entries to filter out any sentence within each speech that does not share any entry in the dictionary.

Using the trimmed instances (containing sentences related to the problem) they perform standard bag-of-words feature extraction (with uni-grams and bi-grams) along with feature selection. For feature selection they disregarded any word that occurred in more than 75\% of the instances as well as in only 1\% of the instances. Furthermore they used chi-square test to remove further insignificant words leading us to a feature vector containing 1500 words.

For each instance they extracted a feature vector containing those significant 1500 terms. The feature values are simple word counts. They used linear SVM regression model, where the outcome is the true score, with parameter tuning. The model is capable to score each instance between 1 (pro EU) and 0 (non pro EU). As a number of speech instances are coming from the affiliates of one party, there are a number of predictive scores for each party. They use the median as the final predictive score. For comparison purposes they repeated the experiments without the filtering process, i.e. feature vectors were extracted without removing any sentence. However, they applied the same feature selection as performed with the dictionary filtering case. They refer to this last experiment as "without dictionary" and the former experiment as "with dictionary". Against their intuition, the obtained results show that the inclusion of all datasets and sentences performs better on the task than filtering the sentences. This needs further investigation in order to improve and adopt the dictionary to the task
